% -*- TeX-engine: luatex -*-
\RequirePackage[l2tabu,orthodox]{nag}

\documentclass[aspectratio=169]{beamer}
\usetheme{nvidia}

\usepackage{booktabs}
\title{The modern CUDA ecosystem}
\subtitle{how to talk to your GPU}
\author{Lawrence Mitchell}
\institute{\texttt{lmitchell@nvidia.com}}
\usepackage{soul}

\usepackage[url=false,
doi=true,
isbn=false,
style=authoryear,
maxnames=5,
giveninits=true,
uniquename=init,
backend=biber]{biblatex}
\renewcommand{\bibfont}{\fontsize{7}{7}\selectfont}
\addbibresource{09-firedrake.bib}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Prehistory}
  \begin{quote}
    The contributions of this paper are: a high-performance
    implementation of a finite element solver for an
    advection-diffusion problem written using NVidia CUDA (Section 3),
    a prototype implementation of a compiler that generates CUDA code
    from UFL sources (Section 4.2), [\dots]
  \end{quote}
  \pause
  {
    \scriptsize
    \raggedleft
    Markall, Ham, and Kelly.
    \emph{Towards generating optimised finite
      element solvers for GPUs from high-level specifications},
    Procedia Computer Science (2010) \nocite{Markall:2010}
    \par
    GTX 480, 1.5GB @ 177GB/s, 1.3TFLOP/s (32bit), 168GFLOP/s (64bit)
    \par
  }
\end{frame}
\begin{frame}
  \frametitle{Prehistory}
  \begin{quote}
    [W]e present finite element implementations that are
    written in both CUDA, for NVidia GPUs, and OpenCL, [\dots We] show
    that the point at which it becomes profitable to switch algorithms
    differs between target architectures. Using OpenCL achieves
    functional portability across different multicore and GPU
    platforms; we demonstrate that performance portability is more
    challenging to achieve.
  \end{quote}
  \pause
  {
    \scriptsize
    \raggedleft
    Markall, Slemmer, Ham, Kelly, Cantwell, and Sherwin.
    \emph{Finite element assembly strategies on multi-core and many-core architectures},
    Int.~J.~Num.~Meth.~Fluids (2012) \nocite{Markall:2013a}
    \par
    \vspace{\baselineskip}
    GTX 480, 1.5GB @ 177GB/s, 1.3TFLOP/s (32bit), 168GFLOP/s (64bit)
    \par
  }
\end{frame}

\begin{frame}
  \frametitle{\textcolor{black!40}{Pre}history}
  \begin{quote}
    PyOP2 provides an API for the implementation of target-specific
    backends. At present, backends for sequential C code, C with
    OpenMP, and CUDA code are fully supported.
  \end{quote}
  \pause
  {
    \scriptsize
    \raggedleft
    Markall, Rathgeber, Mitchell, Loriant, Bertolli, Ham, and Kelly.
    \emph{Performance-portable finite element assembly using PyOP2 and FEniCS},
    ISC (2013) \nocite{Markall:2013}
    \par
    Tesla M2050, 3GB @ 148GB/s, 1TFLOP/s (32bit), 515GFLOP/s (64bit)
    \par
  }
\end{frame}

\begin{frame}
  \frametitle{History}
  \begin{quote}
    This paper presents only performance results [\dots] on CPUs
    [\dots The] available hybrid parallel and GPU linear solver
    libraries are far more limited than PETScâ€™s MPI-only functionality
    [\dots we] have therefore given priority to achieving high
    performance and feature completeness for the CPU backend using
    MPI.\@
  \end{quote}
  \pause
  {
    \scriptsize
    \raggedleft
    Rathgeber, Ham, Mitchell, Lange, Luporini, McRae, Bercea, Markall,
    and Kelly. \emph{Firedrake: automating the finite element method
      by composing abstractions}, ACM TOMS (2016) \nocite{Rathgeber:2016}
    \par
  }
\end{frame}

\begin{frame}
  \frametitle{History}
  \begin{quote}
    Remove all code generation backends except for the sequential CPU
    backend [\dots We]
    have now reached reasonable consensus that the existing
    implementations for GPUs are not the right place to start for
    high-performance assembly.
  \end{quote}
  \pause
  {
    \scriptsize
    \raggedleft
    Mitchell. \url{https://github.com/OP2/PyOP2/pull/507}, (2016)
    \par
  }
\end{frame}

\begin{frame}[fragile]
  \frametitle{What about today?}

  \begin{onlyenv}<1>
    \begin{block}{Firedrake}
      \begin{itemize}
      \item Matrix-free multigrid exists
      \item Sparse solvers better supported on GPUs
      \item Maybe it's time for another go
      \end{itemize}
    \end{block}
    \begin{block}{Premise}
      \begin{itemize}
      \item PETSc is still the foundation of the solver interaction
      \item Firedrake would need to develop GPU-enabled \verb~assemble~
      \end{itemize}
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{block}{GPUs}
      \begin{itemize}
      \item Data center GPUs have \emph{quite a lot} of RAM
        \begin{itemize}
        \item H100 PCIe: 80GB @ 2TB/s, 51.2TFLOP/s (32bit),
          51.2TFLOP/s (64bit)
        \end{itemize}
      \item They are both easier and harder to program
        \begin{itemize}
        \item Automatic caches
        \item There is a memory model!
        \item Just SIMT is not enough for peak performance
        \end{itemize}
      \end{itemize}
    \end{block}
  \end{onlyenv}
\end{frame}

% \begin{frame}
%   \frametitle{Programming model: SIMT}
%   \begin{block}{'twas ever thus}
%     \begin{itemize}
%     \item Broadly unchanged since Ian Buck's 2006 thesis
%     \item Threads grouped into warps (32 threads) and thread blocks
%     \item Communication within a thread block is fast
%     \item Communication between thread blocks is
%       ``slow''
%     \item \dots \emph{but}, can't target the tensors cores this way
%     \end{itemize}
%   \end{block}
% \end{frame}

\begin{frame}
  \frametitle{Tensor cores}
  \begin{columns}[t]
    \begin{column}{0.5\textwidth}
      \begin{block}{What}
        \begin{itemize}
        \item Hardware/instructions for matrix-matrix accumulate: $C \gets A B + C$
        \item Zoo of different precisions
        \item \emph{Collective} across a warp
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{block}{Why: power wall}
        \begin{itemize}
        \item Fewer instructions
        \item Data access: wider loads/stores
        \item Energy cost of multiplication goes like
          square of mantissa bits
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Energy costs}
  \begin{table}
    \centering
    \begin{tabular}{ccc}
      \toprule
      Precision & Operation & Energy/FLOP \\
      \midrule
      FP64 & FMA & 2.5 \\
      FP32 & FMA & 1.0 \\
      FP16 & FMA & 0.5 \\
      FP64 & MMA & 1.5 \\
      FP16 & MMA & 0.12 \\
      FP8 & MMA & 0.06 \\
      \bottomrule
    \end{tabular}
    \caption{Energy cost (arbitrary units) per FLOP for $C \gets A B + C$
      on Hopper}
  \end{table}
\end{frame}

\begin{frame}
  \frametitle{Algorithms: back to the 60s}
  \begin{itemize}
  \item My algorithm needs 64bit/32bit precision, what do all these fancy
    formats help me?
  \item Iterative refinement everywhere!\\
    {\scriptsize
      \raggedleft
      Haidar, Bayraktar, Tomov, Dongarra, Higham, Proc.~R.~Soc.~A
      (2020) \nocite{Haidar:2020}
      \par}
  \item Emulate high-precision scalar with low-precision tensor\\
    {\scriptsize
      \raggedleft
      Ozaki, Ogita, Oishi, Rump, Num.~Alg.~(2012) \nocite{Ozaki:2012}\\
      Ootomo, Ozaki, Yokota. Int.~J.~HPCA (2024) \nocite{Ootomo:2024}
      \par}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{What does Firedrake need?}
  \begin{block}{Codegen: TSFC/PyOP\st{2}3}
    \begin{itemize}
    \item Fast small(-ish) matrix-matrix
    \item Cooperative thread parallelism \emph{within} an element
    \item \dots especially for structured elements?
    \item Code gen needs to be tensor-contraction aware
    \item Fast gather/scatter: copy engines on newer hardware
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Library ecosystem to help}
  \begin{onlyenv}<1-2>
    Encapsulate ``speed of light'' performance for computational
    primitives. 
    \pause

    \begin{block}{Old guard}
      \begin{itemize}
      \item thrust: host side \verb~<algorithm>~

        {\scriptsize
          \raggedleft
          Bell and Hoberock. App. GPU Computing (2012) \nocite{Bell:2012}
          \par
        }
      \item CUB:~device side building blocks

        {\scriptsize
          \raggedleft
          Merrill and Garland, Tech Report NVR-2016-002 (2016) \nocite{Merrill:2016}
          \par
        }
      \item Spec-sheet throughput via algorithmic choice
      \item \dots and architecture-specific autotuning for last-mile
      \end{itemize}
    \end{block}
  \end{onlyenv}
  
  \begin{onlyenv}<3>
    \begin{block}{Newer: device side libraries, JIT}
      Idea: bring CUB approach to broader set of use cases
      \begin{itemize}
      \item \alert{cooperative groups}: thread block decompositions and sync
        primitives
      \item \alert{cuBLASDx}: device side BLAS
      \item CUTLASS:~device side ``BLIS''
      \item libcu++: device side C++ standard library
      \item cuCollections: device side hash tables
      \item NVRTC/nvJitLink: runtime compilation, JIT linking/LTO
      \item \dots
      \end{itemize}
    \end{block}
  \end{onlyenv}
\end{frame}

\begin{frame}[fragile]
  \frametitle{cooperative groups}

  \begin{onlyenv}<1-2>
    \begin{block}{What's wrong with this code?}
\begin{minted}{cuda}
__device__ int sum(int *x, int n) {
  ...;
  __syncthreads();
  ...;
}
\end{minted}
      \pause
\begin{minted}{cuda}
__global__ void kernel(int *x, int n) {
  if (threadIdx.x < blockDim.x / 2) {
    // oops, implicit contract that everyone
    // in the block calls me is violated
    sum(x, n);
  }
}
\end{minted}
    \end{block}
  \end{onlyenv}
  \pause
  \begin{onlyenv}<3>
\begin{minted}{cuda}
__device__ int sum(thread_block comm, int *x, int n) {
   ...;
   comm.sync();
   ...;
}

__global__ void kernel(int *x, int n) {
  auto comm = this_thread_block();
  // Can't diverge here by construction
  sum(comm, x, n);
}
\end{minted}
  \end{onlyenv}
  \begin{onlyenv}<4>
    \begin{block}{Idea}
      \begin{itemize}
      \item Historically \verb~__syncthreads()~
        \begin{itemize}
        \item Which threads? All those in a thread block
        \item Was that correct? Hard to say
        \end{itemize}
      \item \verb~cooperative_groups~ introduces communicator-like
        concepts describing sets of threads
        \begin{itemize}
        \item Collective API, barriers, broadcasts
        \item warp-sized groups offer intra-warp shuffles/voting
        \item \dots
        \end{itemize}
      \end{itemize}
    \end{block}
    {
      \scriptsize
      \raggedleft
      \url{https://developer.nvidia.com/blog/cooperative-groups/}
      \par
    }
  \end{onlyenv}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Device side libraries}
  \begin{onlyenv}<1>
    \begin{block}{Everyone knows}
      \begin{itemize}
      \item Kernel fusion is key
      \item Avoids launch latency, better cache reuse
      \item Not just on GPUs
        \begin{itemize}
        \item Build to order BLAS
        \item PETE, Blitz++, numexpr, \dots
        \end{itemize}
      \end{itemize}
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{block}{cuBLASDx}
      \begin{itemize}
      \item Build to order GEMMs in \verb~__device__~ code.
      \item Interleave with rest of your kernel
      \item Works on data in shared memory
      \item Utilities for pack/unpack with layout reshaping
      \item Plausibly good fit for change-of-basis tensor contractions in TSFC
      \end{itemize}
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<3>
\begin{minted}[mathescape=true]{cuda}
using namespace cublasdx;
// primitive for $C_{mn} \gets \alpha A_{mk} B_{kn} + \beta C_{mn}$, m, n, k compile-time known
// specialised for sm_90 (Hopper)
using BLAS = decltype(Size<m, n, k>()
  + Precision<float>() + Type<real>() + SM<900>() + ...)

__device__ void kernel(float *A, float *B, float *C, ...) {
   ...; 
   cublasdx::copy<BLAS, alignment::a>(A, a_smem);
   ...;
   BLAS().execute(alpha, a_smem, b_smem, beta, c_smem);
   ...;
}
\end{minted}
    {
      \scriptsize
      \raggedleft
      \url{https://docs.nvidia.com/cuda/cublasdx/}
      \par
    }
  \end{onlyenv}
 %  \begin{onlyenv}<4>
 %    \begin{block}{CUTLASS}
 %      \begin{itemize}
 %      \item Building blocks with lower-level control than cuBLASDx
 %      \item Specialises for architecture
  
 %      \item emits tensor-core operations where possible, scalar code
 %        if not
 %      \item \dots this is arch/matrix size-specific, best not to do it
 %        yourself
 %      \item Also warp producer-consumer pattern for Hopper onwards
 %      \end{itemize}
 %    \end{block}
 %    {
 %    \scriptsize
 %    \raggedleft
 %    \url{https://github.com/NVIDIA/cutlass/blob/main/media/docs/code_organization.md}
 %    \par
 %  }
 %  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Why so complicated?}
  \begin{itemize}
  \item SIMT model doesn't fit to tensor cores
  \item Probably no hope that a parallelising compiler will get you
    there
  \item \dots 50-odd years of trying hasn't managed it yet
  \item Compile-time specialisation rules the roost
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Not touched on}
  \begin{block}{Library-level interoperability}
    \begin{itemize}
    \item Mostly OK, some rough edges with MPI
    \item \dots MPI-5 (6?) might help
    \item I don't like implicit ``current device'' concept
    \item \dots modernisation efforts here
    \end{itemize}
  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Where to go for more}
  \begin{itemize}
  \item Many best practices written up in technical blogs
  \item Or presentations at GTC
  \end{itemize}
  \begin{block}{Some pointers}
    \begin{itemize}
    \item \url{docs.nvidia.com/cuda/cuda-c-best-practices-guide/}
    \item Intro to CUDA performance optimisation: \url{nvidia.com/en-us/on-demand/session/gtc24-s62191/}
    \item Early days: \url{github.com/NVIDIA/accelerated-computing-hub/}
    \end{itemize}
  \end{block}
\end{frame}
\begin{frame}
  \begin{center}
    We have open roles in these, and related areas, come speak to me!
  \end{center}
\end{frame}

\begin{frame}[t,allowframebreaks]
  \frametitle{References}
  \printbibliography[heading=none]
\end{frame}


% \begin{abstract}
%   Firedrake emerged, in part, from projects looking at finite elements
%   on GPUs, but that was in the depths of history and we swiftly
%   pivoted to a CPU-centric approach.

%   A decade later, there is interest in revisiting this decision, and
%   both the hardware and software have moved on since the last time
%   Firedrake tried this in anger. As well as a much broader ecosystem
%   of GPU libraries, there is also renewed interest in making the core
%   programming experience harder to misuse. To that end, I'll present
%   some thoughts on device-side libraries and programming abstractions
%   for writing fast kernels correctly.
% \end{abstract}
\end{document}
